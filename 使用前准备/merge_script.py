#æ­¤è„šæœ¬ç”¨äºåˆå¹¶ä¸‹è½½å¥½çš„è®ºå›jsonæ–‡ä»¶
import os
import json
import glob
import re
import csv
import time

# ================= é…ç½®åŒºåŸŸ =================
# 1. ä½ çš„ CSV æ–‡ä»¶è·¯å¾„
CSV_FILE_PATH = "all_threads.csv"

# 2. ä½ çš„ JSON å¤‡ä»½æ–‡ä»¶å¤¹è·¯å¾„ (æ”¯æŒé€’å½’)
JSON_FILES_PATTERN = "\backup\**\*.json"

# 3. è¾“å‡ºæ–‡ä»¶å
OUTPUT_FILENAME = "merged_final_2025.json"

# 4. ä½ çš„ç›®æ ‡åˆ†ç±» ID (ç”¨äºæ„å»ºå‡çš„é¢‘é“ä¿¡æ¯)
TARGET_CATEGORY_ID = "1019924310665728022"


# ===========================================

def get_id_from_filename(filename):
    """ä»æ–‡ä»¶åä¸­æå– IDï¼Œä¾‹å¦‚ '...[123456].json' -> '123456'"""
    # åŒ¹é…æ–‡ä»¶åæœ«å°¾çš„ [æ•°å­—].json
    match = re.search(r'\[(\d+)\]\.json$', filename)
    if match:
        return match.group(1)
    return None


def merge_with_csv_logic():
    print(f"ğŸš€ å¯åŠ¨ï¼æ­£åœ¨è¯»å– CSV ç´¢å¼•: {CSV_FILE_PATH} ...")

    # --- ç¬¬ä¸€æ­¥ï¼šè¯»å– CSV æ„å»ºç™½åå• ---
    valid_ids = set()
    try:
        with open(CSV_FILE_PATH, 'r', encoding='utf-8') as f:
            reader = csv.DictReader(f)
            for row in reader:
                if row.get('id'):
                    valid_ids.add(row['id'])
    except Exception as e:
        print(f"âŒ è¯»å– CSV å¤±è´¥: {e}")
        return

    print(f"ğŸ“‹ CSV è¯»å–å®Œæ¯•ï¼Œå…±åŒ…å« {len(valid_ids)} ä¸ªæœ‰æ•ˆå¸–å­ IDã€‚")
    print("-" * 30)

    # --- ç¬¬äºŒæ­¥ï¼šæ‰«ææ–‡ä»¶å¹¶åˆ©ç”¨æ–‡ä»¶åå¿«é€Ÿè¿‡æ»¤ ---
    print(f"ğŸ” æ­£åœ¨æ‰«ææ–‡ä»¶: {JSON_FILES_PATTERN} ...")
    all_files = glob.glob(JSON_FILES_PATTERN, recursive=True)

    print(f"ğŸ“Š ç¡¬ç›˜ä¸Šå…±æ‰¾åˆ° {len(all_files)} ä¸ª JSON æ–‡ä»¶ã€‚")
    print("âš¡ å¼€å§‹æ–‡ä»¶ååŒ¹é…ä¸æ¶ˆæ¯åŠ è½½...")

    all_messages = []
    header_info = None  # ç”¨äºå­˜å‚¨å¤´éƒ¨ä¿¡æ¯

    processed_count = 0
    skipped_by_csv = 0
    start_time = time.time()

    for index, filepath in enumerate(all_files):
        # æ’é™¤è¾“å‡ºæ–‡ä»¶è‡ªå·±
        if filepath.endswith(OUTPUT_FILENAME): continue

        # 1. ä»æ–‡ä»¶åæå– ID
        filename = os.path.basename(filepath)
        file_id = get_id_from_filename(filename)

        # 2. CSV æ¯”å¯¹ (æ ¸å¿ƒä¼˜åŒ–ï¼šæ–‡ä»¶åIDä¸åœ¨CSVé‡Œï¼Œç›´æ¥è·³è¿‡ï¼Œä¸è¯»æ–‡ä»¶å†…å®¹)
        if file_id and file_id not in valid_ids:
            skipped_by_csv += 1
            # print(f"è·³è¿‡: {filename} (ä¸åœ¨CSVä¸­)") # è°ƒè¯•æ—¶å¯æ‰“å¼€
            continue

        # 3. è¯»å–å†…å®¹ (åªæœ‰åŒ¹é…æˆåŠŸçš„æ‰è¯»ï¼ŒèŠ‚çœæ—¶é—´)
        try:
            with open(filepath, 'r', encoding='utf-8') as f:
                data = json.load(f)

                # æå–å¤´éƒ¨ä¿¡æ¯(åªåšä¸€æ¬¡)
                if header_info is None and 'guild' in data:
                    merged_channel = data.get('channel', {})
                    merged_channel['name'] = f"2025åˆå¹¶å­˜æ¡£-{merged_channel.get('category', 'All')}"
                    merged_channel['id'] = TARGET_CATEGORY_ID

                    header_info = {
                        "guild": data.get('guild', {}),
                        "channel": merged_channel,
                        "dateRange": {"after": None, "before": None},
                        "exportedAt": "2026-01-16T00:00:00.0000000+00:00"
                    }

                # æå–æ¶ˆæ¯
                msgs = data.get('messages', [])
                if msgs:
                    all_messages.extend(msgs)

                processed_count += 1

        except Exception as e:
            # é‡åˆ°åæ–‡ä»¶ä¸æŠ¥é”™ï¼Œç›´æ¥è·³è¿‡
            pass

        if (index + 1) % 5000 == 0:
            print(f"â³ è¿›åº¦: æ‰«æ {index + 1} | å‘½ä¸­CSVå¹¶è¯»å–: {processed_count} | CSVè·³è¿‡: {skipped_by_csv}")

    print(f"\nâœ… æ–‡ä»¶è¯»å–å®Œæˆï¼")
    print(f"   - CSV å‘½ä¸­æœ‰æ•ˆæ–‡ä»¶: {processed_count}")
    print(f"   - CSV è¿‡æ»¤æ— å…³æ–‡ä»¶: {skipped_by_csv}")
    print(f"   - å¾…æ’åºæ¶ˆæ¯æ€»æ•°: {len(all_messages)}")

    if not header_info:
        print("âŒ é”™è¯¯ï¼šæ²¡æœ‰è¯»å–åˆ°ä»»ä½•æœ‰æ•ˆçš„ JSON æ•°æ®ï¼Œè¯·æ£€æŸ¥è·¯å¾„æˆ– CSV ID æ˜¯å¦åŒ¹é…ã€‚")
        return

    # --- ç¬¬ä¸‰æ­¥ï¼šå…¨é‡æ¶ˆæ¯æ’åº (è§£å†³æŠ¥é”™çš„æ ¸å¿ƒ) ---
    print(f"\nğŸ”„ æ­£åœ¨å¯¹ {len(all_messages)} æ¡æ¶ˆæ¯æŒ‰æ—¶é—´æ’åº (CPU å…¨åŠ›å·¥ä½œä¸­)...")
    sort_start = time.time()

    # æŒ‰ timestamp å­—ç¬¦ä¸²æ’åº
    all_messages.sort(key=lambda x: x.get('timestamp', ''))

    print(f"âš¡ æ’åºè€—æ—¶: {time.time() - sort_start:.2f} ç§’")

    # --- ç¬¬å››æ­¥ï¼šå†™å…¥æœ€ç»ˆæ–‡ä»¶ ---
    print(f"ğŸ’¾ æ­£åœ¨å†™å…¥æœ€ç»ˆæ–‡ä»¶: {OUTPUT_FILENAME} ...")

    header_info['messages'] = all_messages
    header_info['messageCount'] = len(all_messages)

    with open(OUTPUT_FILENAME, 'w', encoding='utf-8') as f:
        json.dump(header_info, f, ensure_ascii=False, indent=2)

    total_time = time.time() - start_time
    print("=" * 40)
    print(f"ğŸ‰ å®Œç¾åˆå¹¶å®Œæˆï¼")
    print(f"ğŸ“‚ è¾“å‡ºæ–‡ä»¶: {os.path.abspath(OUTPUT_FILENAME)}")
    print(f"â±ï¸ æ€»è€—æ—¶: {total_time:.2f} ç§’")
    print("=" * 40)


if __name__ == "__main__":
    merge_with_csv_logic()